from collections import namedtuple
from functools import partial

ReadUntil = namedtuple('ReadUntil', ('data', 'remaining'))


def iter_chunks(file_or_func, size=1024):
    """
    Read from stream in chunks

    >>> import io
    >>> s = io.BytesIO(b'Foo bar')
    >>> for chunk in iter_chunks(s):
    ...     print(chunk)
    b'Foo bar'

    :param file_or_func: Stream to read from
    :param size: Maximum chunk size (default 1024)
    :type file_or_func: socket|file|callable
    :type size: int
    :returns: Yields chunks
    :rtype: bytes
    """
    func = file_or_func
    if hasattr(file_or_func, 'read'):
        func = file_or_func.read
    elif hasattr(file_or_func, 'recv'):
        func = file_or_func.recv

    # can't use yield from here, cause we want to have py2 support
    for chunk in iter(partial(func, size), b''):
        yield chunk


def read_until(file_or_func, end=b'\n', max_bytes=16*1024**2):
    """
    Read from stream until a delimiter

    >>> import io
    >>> s = io.BytesIO(b'Foo FINISH bar')
    >>> read_until(s, b'FINISH')
    ReadUntil(data=b'Foo ', remaining=b'FINISH bar')

    :rtype: tuple(bytes, bytes)
    """
    data = bytearray()
    start_idx = len(end)
    for chunk in iter_chunks(file_or_func):
        # start search at the beginning of the current chunk
        # minus the length of the end string
        # This catches data like
        # end=b'FINISH'
        # chunk1=foobarFIN
        # chunk2=ISH
        # without search through the whole data at each iteration
        start_idx -= len(end)
        data.extend(chunk)

        idx = data.find(end, start_idx)
        if idx >= 0:
            return ReadUntil(data[:idx], data[idx:])

        start_idx = len(data)

        if start_idx >= max_bytes:
            break

    return ReadUntil(data, b'')
